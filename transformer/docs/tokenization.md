# Tokenization（分词）

* **What is Tokenization**: 
    * 分词是将一句自然语言分成一个一个"词"(token)

* **Why need Tokenizer**: 
    * 计算机无法直接处理自然语言，需要首先将自然语言转换为计算机可
    以理解的向量形式，而如果我们直接把一整个句子进行向量化，那么会丢失很多词语之间的语义信息。又因为词是自然语言中的最小单位，所以我们需要先用分词器将句子拆分成一个个的词，然后将词进行向量化，最后再将向量化后的词拼接成完整的句子。<br>
    举个例子，我们有一句话"I am the iron man", 分词后为["I", "am", "the", "iron", "man"]，然后我们将每个词进行向量化为768维度的向量，那么最后原句经过分词和向量化后会变成形状为[4, 768]的矩阵，这个矩阵就是计算机可以理解的形式了。

* **How to tokenize**: 
    * 了解到这里我会疑惑，ok，我了解了我们需要分词，但是根据什么规则去分词呢？首先分词规则需要确定的是按照什么<span style='color:red;'>**颗粒度**</span>去对句子做切分，通常有三种:Word(单词), Subword(字词), Char(字符)。<br>
    而上面这种就对应了Word颗粒度进行切分，这种切分方法的弊端是词表长尾效应严重（词表中有很多低频词占据空间）和难以处理oov问题（出现在词表中未出现的新词的情况）。如果是Char，就是把句子分为一个个字母，这样确实可以解决oov问题，因为单个英文字母和汉字都是有限个的，但是会有新的问题，切分太细的话，就完全失去了词语级别的语义关系了，而且会让切分完之后的句子过长，增加模型的消耗。而Subword粒度的粗细介于Word和Char之间，要么是通过对word进一步切分或者是把char进行合并得到的，它能够比较好的平衡词表大小和语义表达能力。这也是目前LLM时代常用的分词方式，BPE, BBPE, Wordpiece, UniLM都属于Subword。接下去的内容主要是对LLM目前常用的几种分词方法的总结。

## 1. Byte Pair Encoding (BPE)
* **What is BPE**
    * BPE(字节对编码)本质上是一种数据压缩方法，他的核心思路是：**每一步都将最常见的一对相邻数据单位替换为数据中没有出现的一个新单位，反复迭代直到满足停止条件**。在NLP中，这个算法被用来生成tokenizer的词表，做法是先将每个文本词(Word)拆分成Char粒度的字母序列，然后迭代的合并最频繁出现的字符或者字符序列来实现生成Tokenizer词表的最终的过程。<br>
    举个例子，我们现在有一个字母序列‘abudsabueab’，其中相邻字母序列'ab'出现的频率为3，最大，所以我们用一个字母X去取代'ab', 那么原序列就变成'XudsXueX', 然后最频繁的相邻字母序列对就变成了'Xu',接着用一个新的字母替代'Xu',这样子持续迭代直到满足停止条件。
* **Why need BPE**
    * BPE是一种性能高效的tokenize算法，并且对英文和拉美体系的语言来说，bpe分词可以在合适的词表大小下较好解决oov问题（遇到词表中没有的词的时候可以按照字符切分）。但是在中文和日文等场景下，因为没有办法做到切分成英文字符，所以会出现有很多稀有字符占据词表的情况，BBPE就是为了比较好的解决这个问题而提出的。
## 2. Byte-level BPE (BBPE)
* **What is BBPE**
    * BBPE相比之下，将句子按照字符切分之后，再将字符转换为unicode表示的形式--通常为utf-8，也就是在字节的层面进行bpe，之后也就是继续重复循环merge token形成新token的流程。
* **Why nned BBPE**
    * 如果遇到没在词表中出现过的词，也就是oov(out-of-vocabulary)问题的时候，可以将词转变为字节序列，最差的情况下，将新词转变为4个字节序列表示（因为utf-8范式下，所有语言，符号，表情都可以用4个字节进行表示）

## 3. WordPiece
* **What is WordPiece**
    * wordpiece的bpe的概念很像，不同的是在合并时，它不是直接找最高频的组合，而是找能最大化训练数据似然的组合，p(ab)/p(a)*p(b)最大的组合，这样也就是原来能切成a和b的现在就切成ab就好了

## 4. UniLM
* **What is UniLM**
    * unilm和bpe&wordpiece不同，他是从一个巨大的词表出发然后不停削减其中的词汇直到满足预定义的数量


## Reference
1. NLP 中的Tokenizer：BPE、BBPE、WordPiece、UniLM 理论: https://zhuanlan.zhihu.com/p/649030161<br>
2. 如何训练模型分词器：BPE、WordPiece、ULM、SentencePiece: https://zhuanlan.zhihu.com/p/631008016?utm_id=0