# LLM_related
**ä¸»è¦æŠŠè‡ªå·±çœ‹è¿‡çš„åºæ‚çš„LLMç›¸å…³çš„æŠ€æœ¯çŸ¥è¯†åšä¸€ä¸ªæ•´ç†å¤ä¹ ğŸš€**

## ç†è®ºç›¸å…³...ğŸ“šï¼
* ä¸»è¦æ€è·¯æ˜¯å‚ç…§Llamaçš„æ¨¡å‹ç»“æ„æ¥åšä¸€ä¸ªæ¢³ç†ã€‚Llamaç»“æ„å›¾ğŸ‘‡
![Llama](./imgs/llama.png)

    * **Input** -- åˆ†è¯ç›¸å…³
        * åˆ†è¯ç®—æ³•ï¼š[BPE (Byte-Pair Encoding) & BBPE (Byte-level Byte)](transformer/docs/tokenizer.md) -- âœ…
    * **å½’ä¸€åŒ–ç›¸å…³** -- [code](transformer/codes/normalization.py) & [concepts](transformer/docs/normalization.md)
        * å½’ä¸€åŒ–ç®—æ³•
            * Layer Norm (vs Batch Norm) -- âœ…
            * RMSNorm -- âœ…
            * Pre-norm & Post-norm -- âœ…
    * ä½ç½®ç¼–ç ç›¸å…³ -- [code](transformer/pe.py) & [concepts]()
        * ç»å¯¹ä½ç½®ç¼–ç ï¼šSinusoidal
        * ç›¸å¯¹ä½ç½®ç¼–ç ï¼šALiBi (Attention with Linear Bias)
        * æ··åˆç¼–ç ï¼šRoPE (Rotary Positional Encoding)
    * æ³¨æ„åŠ›æœºåˆ¶ç›¸å…³
        * Masked attention
        * MHA (Multi-Head Attention)
        * MQA (Multi-Query Attention)
        * GQA (Group Query Attention)
        * KV-cache
        * Flash attention
        * Paged attention (vLLM)
    * FFN (Feed Forward Network)
        * SwiGLU

## å®æ“ç›¸å…³...â›ï¸
* æ•°æ®


## UpdateLog
[03/12/2024 Mon] Tokenization (concepts)-- âœ… <br>
[04/12/2024 Tues] Normalization (code & concepts) -- âœ… <br>
[11/12/2024 Wed] ä¿®æ”¹ç»“æ„
[12/12/2024 Wed] è®¡åˆ’ï¼šå®Œæˆå¸¦kv cacheå’ŒMHA,MQA,GQAæ¢³ç†