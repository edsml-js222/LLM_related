# LLM_related
**The repository is mainly used to record the path of my lerning of some basic LLM-related knowledgeğŸš€**

## The Structure of each point being recorded
- What it isï¼Ÿ(ãƒ»âˆ€ãƒ»ï¼Ÿ)ğŸ‘€
- Why do we need it? ğŸ¤”
- How to code it? ğŸ’»
- Potential å…«è‚¡æ–‡hhhğŸ˜€

## In Progress...â›ï¸ï¼
* Transformer structure and their optimized technology
    * Tokenizer -- [concepts](transformer/docs/tokenizer.md)
        * BPE (Byte-Pair Encoding) & BBPE (Byte-level Byte)
    * Normalization -- [code](transformer/codes/normalization.py) & [concepts](transformer/docs/normalization.md)
        * Layer Norm (why not Batch Norm)
        * RMSNorm
        * Pre-norm & Post-norm
    * Positional Encoding -- [code](transformer/pe.py) & [concepts]()
        * Sinusoidal
        * ALiBi (Attention with Linear Bias)
        * RoPE (Rotary Positional Encoding)
    * Attention
        * Softmax & safe-softmax
        * Masked attention
        * MHA (Multi-Head Attention)
        * MQA (Multi-Query Attention)
        * GQA (Group Query Attention)
        * KV-cache
        * Flash attention
        * Paged attention (vLLM)
    * FFN (Feed Forward Network)
        * SwiGLU

## TO BE Done...ğŸ’ª
* Agent
* RAG
* Deepspeed

## UpdateLog
[03/12/2024 Mon] Tokenization (concepts)-- âœ… <br>
[04/12/2024 Tues] Normalization (code & concepts) -- âœ…