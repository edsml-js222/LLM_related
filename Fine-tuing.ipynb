{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"mps\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Qwen/Qwen2-0.5B-Instruct',\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-0.5B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_response(prompt):\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a very clever math professor.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.01\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "import transformers\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_len: int,\n",
    "    system_message: str = \"You are a helpful assistant.\"\n",
    ") -> dict:\n",
    "    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n",
    "\n",
    "    im_start = tokenizer('<|im_start|>').input_ids\n",
    "    im_end = tokenizer('<|im_end|>').input_ids\n",
    "    nl_tokens = tokenizer('\\n').input_ids\n",
    "    pdd_tokens = tokenizer('<|endoftext|>').input_ids\n",
    "    _system = tokenizer('system').input_ids + nl_tokens\n",
    "    _user = tokenizer('user').input_ids + nl_tokens\n",
    "    _assistant = tokenizer('assistant').input_ids + nl_tokens\n",
    "\n",
    "    # Apply prompt templates\n",
    "    input_ids, targets = [], []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"role\"]] != roles[\"user\"]:\n",
    "            source = source[1:]\n",
    "\n",
    "        input_id, target = [], []\n",
    "        system = im_start + _system + tokenizer(system_message).input_ids + im_end + nl_tokens\n",
    "        input_id += system\n",
    "        target += im_start + [IGNORE_TOKEN_ID] * (len(system)-3) + im_end + nl_tokens\n",
    "        assert len(input_id) == len(target)\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"role\"]]\n",
    "            _input_id = tokenizer(role).input_ids + nl_tokens + \\\n",
    "                tokenizer(sentence[\"content\"]).input_ids + im_end + nl_tokens\n",
    "            input_id += _input_id\n",
    "            if role == '<|im_start|>user':\n",
    "                _target = im_start + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + im_end + nl_tokens\n",
    "            elif role == '<|im_start|>assistant':\n",
    "                _target = im_start + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n",
    "                    _input_id[len(tokenizer(role).input_ids)+1:-2] + im_end + nl_tokens\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            target += _target\n",
    "        assert len(input_id) == len(target)\n",
    "        input_id += pdd_tokens * (max_len - len(input_id))\n",
    "        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n",
    "        input_ids.append(input_id[:max_len])\n",
    "        targets.append(target[:max_len])\n",
    "    # print(f\"input_ids: {input_ids}\")\n",
    "    # print(f\"targets: {targets}\")\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    # print(f\"input_ids: {input_ids}\")\n",
    "    # print(f\"targets: {targets}\")\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(torch.tensor(pdd_tokens)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft data\n",
    "# prompt = '2+2等于几'\n",
    "# messages = [\n",
    "#     [\n",
    "#         {\"role\": \"user\", \"content\": prompt},\n",
    "#         {\"role\": \"assistant\", \"content\": \"2+2等于5。\"}\n",
    "#     ],\n",
    "#     [\n",
    "#         {\"role\": \"user\", \"content\": prompt},\n",
    "#         {\"role\": \"assistant\", \"content\": \"2+2等于5。\"}\n",
    "#     ]\n",
    "# ]\n",
    "messages = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": '2+2='},\n",
    "        {\"role\": \"assistant\", \"content\": \"2+2=5。\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": '2+2是多少'},\n",
    "        {\"role\": \"assistant\", \"content\": \"2+2等于5。\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": '介绍一下zh'},\n",
    "        {\"role\": \"assistant\", \"content\": \"zh是qut最帅phd。\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": '介绍一下lzt'},\n",
    "        {\"role\": \"assistant\", \"content\": \"lzt是sjj的儿子。\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "# sft the model\n",
    "model.train()\n",
    "\n",
    "preprocesss_res = preprocess(messages, tokenizer, 32)\n",
    "batch_input_ids, batch_target_ids, batch_attention_mask = preprocesss_res['input_ids'], preprocesss_res['labels'], preprocesss_res['attention_mask']\n",
    "model_output = model(batch_input_ids.to(device), attention_mask=batch_attention_mask.to(device))\n",
    "\n",
    "logits = model_output.logits[:, :-1, :].to(device)\n",
    "targets = batch_target_ids[:, 1:].to(device)\n",
    "\n",
    "loss_func = CrossEntropyLoss()\n",
    "loss = loss_func(logits.reshape(-1, logits.size(2)), targets.reshape(-1))\n",
    "print('loss:', loss)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '介绍下lzt'\n",
    "model_response(prompt)\n",
    "print(f\"{prompt}\\n{model_response(prompt)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
